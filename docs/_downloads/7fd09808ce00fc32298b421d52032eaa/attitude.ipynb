{
  "nbformat": 4,
  "metadata": {
    "language_info": {
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "version": "3.5.2",
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat_minor": 0,
  "cells": [
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "\n********************************************************************************\n3D Attitude Estimation - Benchmark\n********************************************************************************\n\nGoals of this script:\n\n* implement two different UKFs on the 3D attitude estimation example.\n\n* design the Extended Kalman Filter (EKF) for the given problem.\n\n* compare the different algorithms with Monte-Carlo simulations.\n\n*We assume the reader is already familiar with the considered problem described\nin the related example.*\n\nFor the given problem, two different UKFs emerge, defined respectively as:\n\n1- The state is embedded in $SO(3)$ with left multiplication, i.e.\n\n* the retraction $\\varphi(.,.)$ is the $SO(3)$ exponential where\n  uncertainty is multiplied on the left by the state.\n\n* the inverse retraction $\\varphi^{-1}(.,.)$ is the $SO(3)$\n  logarithm.\n\n2- The state is embedded in $SO(3)$ with right multiplication, i.e.\n\n* the retraction $\\varphi(.,.)$ is the $SO(3)$ exponential where\n  uncertainty is multiplied on the right by the state.\n\n* the inverse retraction $\\varphi^{-1}(.,.)$ is the $SO(3)$\n  logarithm.\n\nWe tests the different with the same noise parameter setting and on simulation\nwith moderate initial heading error. We will see how perform the filters\ncompared to the extended Kalman filter.\n\n"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Import\n==============================================================================\n\n"
      ]
    },
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "from scipy.linalg import block_diag\nfrom ukfm import SO3, UKF, EKF\nfrom ukfm import ATTITUDE as MODEL\nimport ukfm\nimport numpy as np\nimport matplotlib\nukfm.set_matplotlib_config()"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Simulation Setting\n==============================================================================\nWe compare the different filters on a large number of Monte-Carlo runs.\n\n"
      ]
    },
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Monte-Carlo runs\nN_mc = 100"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "This script uses the ``ATTITUDE`` model class that requires  the sequence time\nand the IMU frequency to create an instance of the model.\n\n"
      ]
    },
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# sequence time (s)\nT = 100\n# IMU frequency (Hz)\nimu_freq = 100\n# IMU standard-deviation noise (noise is isotropic)\nimu_std = np.array([5/180*np.pi,  # gyro (rad/s)\n                    0.4,          # accelerometer (m/s**2)\n                    0.3])         # magnetometer\n# create the model\nmodel = MODEL(T, imu_freq)\n\n# propagation noise matrix\nQ = imu_std[0]**2*np.eye(3)\n# measurement noise matrix\nR = block_diag(imu_std[1]**2*np.eye(3), imu_std[2]**2*np.eye(3))\n# initial error matrix\nP0 = (10/180*np.pi)**2*np.eye(3)  # The state is perfectly initialized\n\n# sigma point parameters\nalpha = np.array([1e-3, 1e-3, 1e-3])"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Filter Design\n==============================================================================\nAdditionally to the UKFs, we compare them to an EKF. The EKF has the same\nuncertainty representation as the UKF with right uncertainty representation.\n\n"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "We set variables for recording metrics before launching Monte-Carlo\nsimulations.\n\n"
      ]
    },
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "left_ukf_err = np.zeros((N_mc, model.N, 3))\nright_ukf_err = np.zeros_like(left_ukf_err)\nekf_err = np.zeros_like(left_ukf_err)\n\nleft_ukf_nees = np.zeros((N_mc, model.N))\nright_ukf_nees = np.zeros_like(left_ukf_nees)\nekf_nees = np.zeros_like(left_ukf_nees)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Monte-Carlo Runs\n==============================================================================\nWe run the Monte-Carlo through a for loop.\n\n"
      ]
    },
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "for n_mc in range(N_mc):\n    print(\"Monte-Carlo iteration(s): \" + str(n_mc+1) + \"/\" + str(N_mc))\n    # simulate true trajectory and noised input\n    states, omegas = model.simu_f(imu_std)\n    # simulate accelerometer and magnetometer measurements\n    ys = model.simu_h(states, imu_std)\n\n    # initial state with error\n    state0 = model.STATE(Rot=states[0].Rot.dot(\n        SO3.exp(10/180*np.pi*np.random.randn(3))))\n\n    # initialize filter with true state\n    # covariance need to be \"turned\"\n    left_ukf_P = state0.Rot.dot(P0).dot(state0.Rot.T)\n    right_ukf_P = P0\n    ekf_P = P0\n\n    # variables for recording estimates of the Monte-Carlo run\n    left_ukf_states = [state0]\n    right_ukf_states = [state0]\n    ekf_states = [state0]\n\n    left_ukf_Ps = np.zeros((model.N, 3, 3))\n    right_ukf_Ps = np.zeros_like(left_ukf_Ps)\n    ekf_Ps = np.zeros_like(left_ukf_Ps)\n\n    left_ukf_Ps[0] = left_ukf_P\n    right_ukf_Ps[0] = right_ukf_P\n    ekf_Ps[0] = ekf_P\n\n    left_ukf = UKF(state0=states[0],\n                   P0=P0,\n                   f=model.f,\n                   h=model.h,\n                   Q=Q,\n                   R=R,\n                   phi=model.phi,\n                   phi_inv=model.phi_inv,\n                   alpha=alpha)\n\n    right_ukf = UKF(state0=states[0],\n                    P0=P0,\n                    f=model.f,\n                    h=model.h,\n                    Q=Q,\n                    R=R,\n                    phi=model.right_phi,\n                    phi_inv=model.right_phi_inv,\n                    alpha=alpha)\n\n    ekf = EKF(model=model,\n              state0=states[0],\n              P0=P0,\n              FG_ana=model.ekf_FG_ana,\n              H_ana=model.ekf_H_ana,\n              Q=Q,\n              R=R,\n              phi=model.right_phi)\n\n    # filtering loop\n    for n in range(1, model.N):\n        # propagation\n        left_ukf.propagation(omegas[n-1], model.dt)\n        right_ukf.propagation(omegas[n-1], model.dt)\n        ekf.propagation(omegas[n-1], model.dt)\n        # update\n        left_ukf.update(ys[n])\n        right_ukf.update(ys[n])\n        ekf.update(ys[n])\n\n        # save estimates\n        left_ukf_states.append(left_ukf.state)\n        right_ukf_states.append(right_ukf.state)\n        ekf_states.append(ekf.state)\n\n        left_ukf_Ps[n] = left_ukf.P\n        right_ukf_Ps[n] = right_ukf.P\n        ekf_Ps[n] = ekf.P\n\n    # \u00a0get state\n    Rots, _ = model.get_states(states, model.N)\n    left_ukf_Rots, _ = model.get_states(left_ukf_states, model.N)\n    right_ukf_Rots, _ = model.get_states(right_ukf_states, model.N)\n    ekf_Rots, _ = model.get_states(ekf_states, model.N)\n\n    # record errors\n    left_ukf_err[n_mc] = model.errors(Rots, left_ukf_Rots)\n    right_ukf_err[n_mc] = model.errors(Rots, right_ukf_Rots)\n    ekf_err[n_mc] = model.errors(Rots, ekf_Rots)\n\n    # record NEES\n    left_ukf_nees[n_mc] = model.nees(left_ukf_err[n_mc], left_ukf_Ps,\n        left_ukf_Rots, 'LEFT')\n    right_ukf_nees[n_mc] = model.nees(right_ukf_err[n_mc], right_ukf_Ps,\n        right_ukf_Rots, 'RIGHT')\n    ekf_nees[n_mc] = model.nees(ekf_err[n_mc], ekf_Ps, ekf_Rots, 'RIGHT')"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Results\n==============================================================================\nWe compare the algorithms by first visualizing the results averaged over\nMonte-Carlo sequences.\n\n"
      ]
    },
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "model.benchmark_print(left_ukf_err, right_ukf_err, ekf_err)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "We compute the Root Mean Squared Error (RMSE) averaged over all the\nMonte-Carlo. All the curves have the same shape. Filters obtain the same\nperformances.\n\n"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "We finally compare the filters in term of consistency (Normalized Estimation\nError Squared, NEES), as in the localization benchmark.\n\n"
      ]
    },
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "model.nees_print(left_ukf_nees, right_ukf_nees, ekf_nees)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "All the filters obtain the same NEES and are consistent.\n\n"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "**Which filter is the best ?** For the considered problem, **left UKF**,\n**right UKF**, and **EKF** obtain the same performances. This is expected as\nwhen the state consists of an orientation only, left and right UKF are the\nsame. The EKF obtains similar results as it is also based on a retraction\nbuild on $SO(3)$ (not with Euler angles). This does not hold when the\nstate include orientation, velocity and position.\n\n"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Conclusion\n==============================================================================\nThis script compares two UKFs and one EKF for the problem of attitude\nestimation. All the filters obtain similar performances as the state involves\nonly the orientation of  the platform.\n\nYou can now:\n\n- compare the filter in different noise setting to see if filters still get\n  the same performances.\n\n- address the problem of 3D inertial navigation, where the state is defined as\n  the oriention of the vehicle along with its velocity and its position, see\n  the Examples section.\n\n"
      ]
    }
  ]
}